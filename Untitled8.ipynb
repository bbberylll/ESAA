{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2SCqWVVoujGRMrancqyMC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbberylll/ESAA/blob/main/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blending Ensemble for Regression Problems\n",
        "\n",
        "https://www.kaggle.com/code/ahmedabdulhamid/blending-ensemble-for-regression-problems\n",
        "\n",
        "\n",
        "## 1. 사용한 데이터 & 주제 설명\n",
        "문제 유형: 회귀(regression)\n",
        "\n",
        "데이터 형식: 수치형 + 범주형 혼합 데이터셋\n",
        "\n",
        "목표: 정형화된 실세계 회귀 문제 해결\n",
        "\n",
        "학습 포인트: 다양한 모델 조합, 블렌딩 구조 설계\n",
        "\n",
        "\n",
        "## 2. 적용된 기법\n",
        "Base 모델: Ridge 회귀 & RandomForest Regressor & XGBoost Regressor & LightGBM Regressor\n",
        "\n",
        "블렌딩 구조:\n",
        "- Hold-out 방식: 데이터를 Train / Hold-out으로 분할하고 Hold-out에서 base 모델 예측 생성\n",
        "- Soft blend: Hold-out 예측을 기반으로 가중 평균 또는 메타 회귀를 사용하여 블렌딩\n",
        "\n",
        "전처리:\n",
        "- 결측치 대체 (평균, 중간값 등)\n",
        "- Label Encoding + Standard Scaling 포함\n",
        "\n",
        "\n",
        "## 3. 문제 접근 방식 요약\n",
        "1. 데이터 분할: Train / Hold‑out (예: 80/20)\n",
        "2. Base 모델 학습: 각 회귀 모델을 Train에 학습한 후 Hold‑out에서 예측값 생성\n",
        "\n",
        "3. 블렌딩: Hold-out 예측치를 기반으로 메타 회귀 (예: 선형 혼합) 또는 단순 가중 평균으로 조합\n",
        "4. 테스트 예측: base 모델의 테스트 예측을 동일한 블렌딩 방정식에 적용 → 최종 예측 생성\n",
        "\n",
        "##  4. 코드 흐름 요약\n",
        "```\n",
        "# 데이터 분할\n",
        "X_train, X_hold, y_train, y_hold = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Base 모델 리스트\n",
        "models = [\n",
        "    ('ridge', Ridge(alpha=1.0)),\n",
        "    ('rf', RandomForestRegressor(n_estimators=100)),\n",
        "    ('xgb', XGBRegressor()),\n",
        "    ('lgbm', LGBMRegressor())\n",
        "]\n",
        "\n",
        "# Hold-out 예측 생성\n",
        "hold_preds = {}\n",
        "for name, model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    hold_preds[name] = model.predict(X_hold)\n",
        "\n",
        "# 블렌딩: 메타 회귀 또는 평균\n",
        "blend_input = np.column_stack([hold_preds[name] for name, _ in models])\n",
        "meta = LinearRegression().fit(blend_input, y_hold)\n",
        "\n",
        "# 테스트 예측\n",
        "test_preds = {}\n",
        "for name, model in models:\n",
        "    test_preds[name] = model.predict(X_test)\n",
        "\n",
        "blend_test_input = np.column_stack([test_preds[name] for name, _ in models])\n",
        "final_pred = meta.predict(blend_test_input)\n",
        "```\n",
        "\n",
        "## 5. 차별점\n",
        "다양한 모델 블렌딩: 선형 vs 트리 기반 회귀 모델의 조화를 통해 예측 안정성 강화\n",
        "\n",
        "Hold‑out 기반 meta 학습으로 overfitting 최소화\n",
        "\n",
        "메타 회귀 사용: 블렌딩 가중치를 자동으로 학습, 단순 평균보다 성능 우위 확보\n",
        "\n",
        "## 6. 5. 배울 점 요약\n",
        "1. 블렌딩 구조\tHold‑out → base 예측 → 메타 모델 학습 방식\n",
        "2. 모델 다양성\t각 모델 특성 살려 보완적 조합 구성\n",
        "3. 자동 가중화\tLinearRegression 기반 가중치 자동 학습\n",
        "4. 과적합 방지\thold‑out 기반 validation으로 안정성 확보"
      ],
      "metadata": {
        "id": "8WRwZ9ltkN0T"
      }
    }
  ]
}